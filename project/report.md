# Creating a Data Pipeline between HDFS and AWS Redshift

Austin Zebrowski, fa19-516-159 :o2:

## Abstract

I will use Apache Airflow to create a data pipeline from Hadoop Distributed File System
(HDFS), to AWS S3, to Amazon Redshift. I will create a RESTful API that exposes data from the Redshift
database.  

## Introduction

TBD

## Design

TBD

* Architecture

## Implementation

* Technologies Used

    1) Apache Airflow (open source, python-based data pipeline)
    2) HDFS
    3) AWS S3
    4) AWS Redshift
    5) Connexion

## Progress

Below you will find weekly report of the progress.

This section to be completed in the following weeks.

## Work Breakdown

All of this work is to be completed by Austin Zebrowski. This is not a group project.

## References

## Results

TBD

* Deployment Benchmarks
* Application Benchmarks
